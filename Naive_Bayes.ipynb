{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nbtrain_w(X, y, no_classes, no_features):\n",
    "   \n",
    "    w = np.zeros((no_classes, no_features))\n",
    "    #index = 0\n",
    "    class1 = []\n",
    "    class2 = []\n",
    "    class3 = []\n",
    "    class4 = []\n",
    "    class5 = []\n",
    "    index=0\n",
    "    for i in y:\n",
    "        if i==1:\n",
    "            class1.append(index)\n",
    "        elif i==2:\n",
    "            class2.append(index)\n",
    "        elif i==3:\n",
    "            class3.append(index)\n",
    "        elif i==4:\n",
    "            class4.append(index)\n",
    "        else:\n",
    "            class5.append(index)\n",
    "        index=index+1\n",
    "            \n",
    "    labels = np.array([class1,class2,class3,class4,class5])\n",
    "    #print X[labels[0]]\n",
    "    total_class1= sum(sum(X[labels[0]]))\n",
    "    #print total_class1\n",
    "    total_class2= sum(sum(X[labels[1]]))\n",
    "    total_class3= sum(sum(X[labels[2]]))\n",
    "    #print total_class2\n",
    "    total_class4= sum(sum(X[labels[3]]))\n",
    "    total_class5= sum(sum(X[labels[4]]))\n",
    "  \n",
    "    #print np.sum(X[labels[0],:])\n",
    "    for i in range(no_features):\n",
    "        sum_freq1 = np.sum(X[labels[0],i])\n",
    "        sum_freq2 = np.sum(X[labels[1],i])\n",
    "        sum_freq3 = np.sum(X[labels[2],i])\n",
    "        sum_freq4 = np.sum(X[labels[3],i])\n",
    "        sum_freq5 = np.sum(X[labels[4],i])\n",
    "        w[0][i] = (sum_freq1 + 1.0) /   (total_class1+no_features+1)\n",
    "        w[1][i] = (sum_freq2 + 1.0) /   (total_class2+no_features+1)\n",
    "        w[2][i] = (sum_freq3 + 1.0) /   (total_class3+no_features+1)\n",
    "        w[3][i] = (sum_freq4 + 1.0) /  (total_class4+no_features+1)\n",
    "        w[4][i] = (sum_freq5 + 1.0) /  (total_class5+no_features+1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_classes = 5\n",
    "no_features = 942\n",
    "\n",
    "w = nbtrain_w(X_train, y_train, no_classes, no_features)\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nbtest(X_test,w):\n",
    "    prob =  np.ones((len(X_test),5))\n",
    "#print len(X_test)\n",
    "#print prob\n",
    "    for index in range(5):#5 classes\n",
    "        for review in range(len(X_test)): #no of test reviews\n",
    "            for feature in range(no_features): #features\n",
    "                if(X_test[review][feature] == 0.0):\n",
    "                    #prob[review][index] = prob[review][index] * (1-w[index][feature])\n",
    "                    continue\n",
    "                else:\n",
    "                    prob[review][index] = prob[review][index] * (w[index][feature] * X_test[review][feature] )\n",
    "    \n",
    "# prob has 6000 rows and 2 cols\n",
    "# test_data_vector has 6000 rows and features cols\n",
    "# w has 2 rows has 2000 cols\n",
    "    #print prob \n",
    "    pclass1=float(sum(y_train==1))/len(y_train)\n",
    "    #print pclass1\n",
    "    pclass2=float(sum(y_train==2))/len(y_train) \n",
    "    #print pclass2\n",
    "    pclass3=float(sum(y_train==3))/len(y_train) \n",
    "    #print pclass3\n",
    "    pclass4=float(sum(y_train==4))/len(y_train) \n",
    "    #print pclass4\n",
    "    pclass5=float(sum(y_train==5))/len(y_train)\n",
    "    #print pclass5\n",
    "    #print len(prob)\n",
    "\n",
    "    for i in range(len(prob)):\n",
    "        np.sum(prob[i][:])\n",
    "        prob[i][0]= (prob[i][0]*pclass1)\n",
    "        prob[i][1]= (prob[i][1]*pclass2)\n",
    "        prob[i][2]= (prob[i][2]*pclass3)\n",
    "        prob[i][3]= (prob[i][3]*pclass4)\n",
    "        prob[i][4]= (prob[i][4]*pclass5)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nbpred(y_test,X_test,w):\n",
    "    prob=nbtest(X_test,w)\n",
    "    pred=np.zeros(len(y_test))\n",
    "    for i in range(len(prob)):\n",
    "        pred[i]= prob[i][:].argmax(axis=0)+1\n",
    "        \n",
    "    #import math\n",
    "    prob_normal=np.zeros((len(prob),1))\n",
    "    #print prob\n",
    "    inf = float(\"inf\")\n",
    "    #print inf\n",
    "    for i in range(len(prob)):\n",
    "        normalizer=np.sum(prob[i][:])\n",
    "        if prob[i][pred[i]-1]== inf or prob[i][pred[i]-1]== -inf :\n",
    "            prob_normal[i]=1\n",
    "        else:\n",
    "            prob_normal[i]=prob[i][pred[i]-1]/normalizer\n",
    "    \n",
    "    for i in range(len(prob)):\n",
    "        if math.isnan(prob_normal[i]):\n",
    "            prob_normal[i]=0\n",
    "    #print prob_normal[1:100]   \n",
    "    return pred,prob_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred,prob_normal=nbpred(y_test,X_test,w)\n",
    "c = confusion_matrix(y_test,pred)\n",
    "print c\n",
    "accuracy=sum(pred==y_test) * 100.0 / len(y_test)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nbpred_ternary(y_test):\n",
    "    #import copy\n",
    "    print y_test[0:5]\n",
    "    y_test_thresholded = np.array(copy.deepcopy(y_test))\n",
    "    #print type(y_test_thresholded)\n",
    "    #print y_test_thresholded\n",
    "    y_test_thresholded[y_test_thresholded < 3] = 0\n",
    "    y_test_thresholded[y_test_thresholded == 3] = 1\n",
    "    y_test_thresholded[y_test_thresholded > 3] = 2\n",
    "    #print  y_test_thresholded\n",
    "    #print id(y_test)\n",
    "    #print(y_test_thresholded)\n",
    "    pred=nbpred(y_test,X_test,w)\n",
    "    pred[pred < 3] = 0\n",
    "    pred[pred == 3] = 1\n",
    "    pred[pred > 3] = 2\n",
    "    accuracy= sum(pred==y_test_thresholded) * 100.0 / len(y_test_thresholded)\n",
    "    return accuracy,pred,y_test_thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy,pred,y_test_thresholded=nbpred_ternary(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(pred, y_test_thresholded)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nbpred_binary(y_test):\n",
    "    import copy\n",
    "    print y_test[0:5]\n",
    "    y_test_thresholded = np.array(copy.deepcopy(y_test))\n",
    "    #print type(y_test_thresholded)\n",
    "    #print y_test_thresholded\n",
    "    y_test_thresholded[y_test_thresholded < 3] = 0\n",
    "    y_test_thresholded[y_test_thresholded == 3] = 1\n",
    "    y_test_thresholded[y_test_thresholded > 3] = 1\n",
    "    #print  y_test_thresholded\n",
    "    #print id(y_test)\n",
    "    #print(y_test_thresholded)\n",
    "    pred,prob_normal=nbpred(y_test,X_test,w)\n",
    "    pred[pred < 3] = 0\n",
    "    pred[pred == 3] = 1\n",
    "    pred[pred > 3] = 1\n",
    "    accuracy= sum(pred==y_test_thresholded) * 100.0 / len(y_test_thresholded)\n",
    "    return accuracy,pred,y_test_thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy1,pred1,y_test_thresholded1=nbpred_binary(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(pred1, y_test_thresholded1)\n",
    "print accuracy1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
